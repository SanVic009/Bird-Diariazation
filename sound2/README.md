# Bird Multi‑Label Diarization (Species: "who sang when")

End‑to‑end PyTorch pipeline to produce **species‑attributed timelines** from long recordings,
even when **multiple birds sing at the same time**.

Your metadata CSV schema (auto‑inspected) has columns:
`primary_label, secondary_labels, type, latitude, longitude, scientific_name, common_name, author, license, rating, url, filename`

We use:
- `primary_label` as the species label
- `filename` as the relative path to audio (e.g. `asbfly/XC267679.ogg`)

> ⚠️ This pipeline assumes your audio files are in `../dataset/train_audio/` and your metadata CSV is at `../dataset/train_metadata.csv`. The paths in `config.yaml` are set up to reflect this. Make sure your data is in the correct location.

## Features
- Frame‑level **multi‑label** classifier (sigmoid) for overlaps
- On‑the‑fly **mix augmentation** to simulate overlaps from single‑species recordings
- Mel‑spectrogram features (no internet/model downloads needed)
- Timeline post‑processing with hysteresis to reduce chatter
- Reproducible splits + label map persisted to disk

## Quickstart
```bash
# 0) Python env (suggested)
python -m venv .venv && source .venv/bin/activate  # on Windows: .venv\Scripts\activate

# 1) Install deps
pip install -r requirements.txt

# 2) Check that your data is in ../dataset/
# Audio should be in ../dataset/train_audio/
# Metadata CSV should be ../dataset/train_metadata.csv

# 3) Create splits + labels
python scripts/prepare_splits.py --metadata "../dataset/train_metadata.csv" --out_dir "../dataset"

# 4) Train
python src/train.py --config config.yaml

# 5) Inference on a file
python src/infer.py --config config.yaml --audio "../dataset/train_audio/asbfly/XC267679.ogg" --out "./outputs"

# 6) Build readable timeline segments (CSV)
python src/make_timeline.py --probs "./outputs/probs_asbfly_XC267679.csv" --out "./outputs/timeline_asbfly_XC267679.csv"
```

## Config
All knobs are in `config.yaml`. Important bits:
- `data.root`: points to `../dataset`
- `audio.sample_rate`, `frame_sec`, `hop_sec`
- `model.n_mels`, `cnn_width`
- `train.mix_prob`, `mix_snr_db`

## BirdNET embeddings (optional)
This repo ships a mel‑spectrogram baseline. If you have BirdNET embeddings precomputed, you can swap the feature extractor in `src/utils/audio.py` or plug embeddings directly in `src/dataset.py`.

---

**Author:** Autogenerated for Dhruvin. No fluff. Just shippable code.
